---
layout: rg_def
title: RG
---

<h1 style="text-align:center"><b>Conversational AI Reading Group</b></h1>
<!-- <img src="images/tgl_logo.png" class="rounded mx-auto d-block" alt="TGL logo"> -->
<div style="text-align: center;">
  <img src="assets/profile-pics/rg_logo2.png" class="img-thumbnail" alt="TGL logo" width="20%" />
</div>

</div>
<h4 style="text-align:center">Every Thursday at 11am-12pm EDT </h4>
<h4 style="text-align:center">
  <i class="fas fa-video" style="margin-right: 5px;"></i>
  Join us via <a href="https://concordia-ca.zoom.us/j/81541793947">Zoom</a>
</h4>
<h5 style="text-align:center">
  <i class="fa-brands fa-x-twitter" style="margin-right: 5px;"></i>
  Follow us on x-Twitter <a href="https://x.com/convAI2024">@convAI2024</a>
</h5>
<h5 style="text-align:center">
  <!-- Currently, there is no specific Bluesky icon in Font Awesome -->
  <i class="fa-brands fa-bluesky" style="margin-right: 5px;"></i> 
  Follow us on Bluesky <a href="https://bsky.app/profile/convai-rg.bsky.social">@convai-rg.bsky.social</a>
</h5>
<h5 style="text-align:center"> 
  <i class="fa-brands fa-youtube"  style="margin-right: 5px;"></i>
  Visit our Youtube Channel for Past Recordings <a href="https://www.youtube.com/@CONVAI_RG"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a> </h5>
<h5 style="text-align:center"> 
  <i class="fa-brands fa-slack"  style="margin-right: 5px;"></i>
  Join the Conversational AI slack to discuss with the community: <a href="https://join.slack.com/t/convairg/shared_invite/zt-2zhzz0w1v-ODqDUJZzzZ6HJQT1L~zxRg">here</a>.<br> Contact <a href="mailto:convai2024@gmail.com">here</a> if there is any issues with the invite link. </h5>

<h5 style="text-align:center">Sign up <a href="https://forms.gle/sUAkfCdfpMwPWiT27">here</a> to receive email communications about the reading group</h5>
<div style="height: 20px;"></div>
<center>
 <a target="_blank" href="https://calendar.google.com/calendar/event?action=TEMPLATE&amp;tmeid=NTZ2NnV0bjEyc2t0YWJmZ3M5Zmtjdjl2bW1fMjAyNDExMTRUMTYwMDAwWiA2OGQ0NmI5MmU4MTJjZjZlOTU0YWMxY2ZiZmJlN2JhMTZkMjI4YTBhZmVhZDFhYWIyZGIzNzE0ZGNiNGJhNDc2QGc&amp;tmsrc=68d46b92e812cf6e954ac1cfbfbe7ba16d228a0afead1aab2db3714dcb4ba476%40group.calendar.google.com&amp;scp=ALL"><img border="0" src="./assets/7123030_google_calendar_icon.png" class="img-thumbnail" alt="TGL logo" width="5%"></a> 
</center>
<div style="height: 20px;"></div>

<section id="schedule" class="some-section">
    <div class="container">
      <div class="row">
    <div class="col-sm-2"></div>
    <div class="col-sm-8">
    <div class="listing" style="clear:both;">
      <div class="left">
        <h3 style="text-align:center"><b>Upcoming Talks</b></h3>

  <!-- <h4>[March 6th, 2025]</h4>
  <li>
    <b>
        <a href=""> TBA </a>
    </b>
    <br> Presenter:<a href="https://www.cs.huji.ac.il/~adiyoss/"><u> Yossi Adi</u></a> Hebrew University and Meta
    <a class="btn btn-info btn-xs" data-toggle="collapse" href="#march6bio" role="button" aria-expanded="false" aria-controls="collapseExample">
     Speaker Bio
    </a>
    <div class="collapse" id="march6bio">
      <div class="card card-body">
        <p>TBA.</p>
      </div>
    </div>
    <br> 
    <a href=""><img src="https://img.shields.io/badge/Paper-link-important"></a> 
    <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#march6" role="button" aria-expanded="false" aria-controls="collapseExample">
      Abstract
      </a>
  <div class="collapse" id="march6">
  <div class="card card-body">
  <p>TBA   </p>     
  </div>
</li> -->
  <h4>[March 13th, 2025]</h4>
  <li>
    <b>
        <a href="https://arxiv.org/abs/2410.00037"> Moshi: a speech-text foundation model for real-time dialogue </a>
    </b>
    <br> Presenter:<a href="https://ai.honu.io/"><u>Alexandre Defossez</u></a> Kyutai
    <a class="btn btn-info btn-xs" data-toggle="collapse" href="#march13bio" role="button" aria-expanded="false" aria-controls="collapseExample">
     Speaker Bio
    </a>
    <div class="collapse" id="march13bio">
      <div class="card card-body">
        <p><u>Alexandre Defossez </u> is a co-founder at Kyutai, a non profit lab for research in artificial intelligence based in Paris. Kyutai's mission is to lead bleeding edge research and to make it accessible through open science and open source. We released the speech-to-speech conversational AI Moshi, and recently Hibiki, the first simultaneous speech translation model that can run on a phone. Before that, Alexandre was a scientist for 3 years at Facebook AI Research in Paris, where he led the development for models for audio compression and modeling (AudioCraft, MusicGen, EnCodec). He graduated in mathematics from École Normale Supérieure, and did his PhD between INRIA and FAIR Paris on music source separation.</p>
      </div>
    </div>
    <br> 
    <a href="https://arxiv.org/abs/2410.00037"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
    <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#march13" role="button" aria-expanded="false" aria-controls="collapseExample">
      Abstract
      </a>
  <div class="collapse" id="march13">
  <div class="card card-body">
  <p>We will discuss Moshi, our recently released model. Moshi is capable of full-duplex dialogue, e.g. it can both speak and listen at any time, offering the most natural speech interaction to date. Besides, Moshi is also multimodal, in particular it is able to leverage its inner text monologue to improve the quality of its generation. We will cover the design choices behind Moshi in particular the efficient joint sequence modeling permitted by RQ-Transformer, and the use of large scale synthetic instruct data. Finally, I will discuss how this approach can be extended to new use cases like simultaneous speech translation.</p>     
  </div>
  </li>

  <h4>[March 20th, 2025]</h4>
  <li>
    <b>
        <a href="https://arxiv.org/abs/2411.19842"> Making transformers work for audio coding </a>
    </b>
    <br> Presenter:<a href=""><u>Julian Parker</u></a> Stability AI
    <a class="btn btn-info btn-xs" data-toggle="collapse" href="#march20bio" role="button" aria-expanded="false" aria-controls="collapseExample">
     Speaker Bio
    </a>
    <div class="collapse" id="march20bio">
      <div class="card card-body">
        <p><u>Julian Parker</u>, born in the UK, holds a B.A. (Hons.) in natural sciences from the University of Cambridge (2005), UK, an M.Sc. in acoustics & music technology from the University of Edinburgh (2008), UK, and a D.Sc. (Tech.) in audio signal processing and acoustics from Aalto University, Finland (2013). His doctoral work focused on computational modelling of dispersive physical systems in the audio frequency range. He has held leadership positions in industrial research at companies such as Native Instruments, TikTok and Stability AI, focusing on processing and generating of musical sound using both traditional signal processing techniques, machine learning and AI. His current research interests are in generative modelling of musical audio, audio coding, and in the intersection between signal processing and neural network structures.</p>
      </div>
    </div>
    <br> 
    <a href="https://arxiv.org/abs/2411.19842"><img src="https://img.shields.io/badge/Paper-link-important"></a>
    <a href="https://stability-ai.github.io/stable-codec-demo/"><img src="https://img.shields.io/badge/website-blue"></a> 
    <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#march20" role="button" aria-expanded="false" aria-controls="collapseExample">
      Abstract
      </a>
  <div class="collapse" id="march20">
  <div class="card card-body">
  <p>Neural audio codec models are crucial to modern generative AI pipelines, but until recently have largely relied on convolutional backbones. In this talk I share some recent work on building and scaling a transformer-based neural audio codec for speech. I’ll discuss my motivations for taking this approach, the challenges encountered along the way, and the successes and limitations of the work.</p>     
  </div>
  </li>
  <h4>[March 27th, 2025]</h4>
  <li>
    <b>
        <a href="https://arxiv.org/abs/2409.11228"> Learning Source Disentanglement in Neural Audio Codec </a>
    </b>
    <br> Presenter:<a href="https://xiaoyubie1994.github.io/"><u>Xiaoyu Bie</u></a> Télécom Paris
    <a class="btn btn-info btn-xs" data-toggle="collapse" href="#march27bio" role="button" aria-expanded="false" aria-controls="collapseExample">
     Speaker Bio
    </a>
    <div class="collapse" id="march27bio">
      <div class="card card-body">
        <p><u>Xiaoyu Bie</u> is currently a postdoctoral researcher at Télécom Paris, Institut Polytechnique de Paris. His research focuses on generative models and audio signal processing.  He is involved in the ERC project Hi-Audio, which aims to developing interpretable and controllable deep neural models for audio by integrating domain-specific knowledge. Previously, he completed his PhD in Computer Science at INRIA and Université Grenoble-Alpes in 2023. He serves as a reviewer for international conferences and journals such as NeurIPS, CVPR, ICASSP, Interspeech and IEEE/ACM TASLP.</p>
      </div>
    </div>
    <br> 
    <a href="https://arxiv.org/abs/2409.11228"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
    <a href="https://xiaoyubie1994.github.io/sdcodec/"><img src="https://img.shields.io/badge/website-blue"></a> 
    <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#march27" role="button" aria-expanded="false" aria-controls="collapseExample">
      Abstract
      </a>
  <div class="collapse" id="march27">
  <div class="card card-body">
  <p>Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.</p>     
  </div>
  </li>
    </div>
  </div>
</section>


<section id="winter2025" class="some-section">
  <div class="container">
    <div class="row">
      <div class="col-sm-2"></div>
      <div class="col-sm-8">
        <div class="listing" style="clear:both;">
        <div class="left">

      <h3 style="text-align:center">Past Talks, Winter 2025</h3>
      <h4>[Feb 27th, 2025]</h4>
      <li>
        <b>
            <a href="https://arxiv.org/abs/2401.16658"> Open Whisper-Style Speech Models: Transparency, Scalability, and Advancing Explainability </a>
        </b>
        <br> Presenter:<a href="https://sites.google.com/view/shinjiwatanabe"><u> Shinji Watanabe</u></a> Carnegie Mellon University
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#feb27bio" role="button" aria-expanded="false" aria-controls="collapseExample">
         Speaker Bio
        </a>
        <div class="collapse" id="feb27bio">
          <div class="card card-body">
            <p><u>Shinji Watanabe</u> is an Associate Professor at Carnegie Mellon University, Pittsburgh, PA. He received his B.S., M.S., and Ph.D. (Dr. Eng.) degrees from Waseda University, Tokyo, Japan. He was a research scientist at NTT Communication Science Laboratories, Kyoto, Japan, from 2001 to 2011, a visiting scholar at Georgia Institute of Technology, Atlanta, GA, in 2009, and a senior principal research scientist at Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA USA from 2012 to 2017. Before Carnegie Mellon University, he was an associate research professor at Johns Hopkins University, Baltimore, MD, USA, from 2017 to 2020. His research interests include automatic speech recognition, speech enhancement, spoken language understanding, and machine learning for speech and language processing. He has published over 500 papers in peer-reviewed journals and conferences and received several awards, including the best paper award from ISCA Interspeech in 2024. He is a Senior Area Editor of the IEEE Transactions on Audio Speech and Language Processing. He was/has been a member of several technical committees, including the APSIPA Speech, Language, and Audio Technical Committee (SLA), IEEE Signal Processing Society Speech and Language Technical Committee (chair, SLTC), and Machine Learning for Signal Processing Technical Committee (MLSP). He is an IEEE and ISCA Fellow.
            </p>
          </div>
        </div>
        <br> 
        <a href="https://arxiv.org/abs/2401.16658"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://aclanthology.org/2024.acl-long.549.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://www.wavlab.org/activities/2024/owsm/"><img src="https://img.shields.io/badge/website-blue"></a> 
        <a href="https://www.youtube.com/watch?v=X9bVfXyoKxc"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/shinji_watanabe_v2.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>  
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#feb27" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract
          </a>
      <div class="collapse" id="feb27">
      <div class="card card-body">
      <p>Speech foundation models are transforming research by unifying speech-processing tasks through scaling data, model size, and task diversity. This shift has divided research roles, with large tech companies building foundational models and smaller entities focusing on refinement and analysis, raising concerns about explainability due to limited transparency. To address this, our group has developed Open Whisper-style Speech Models (OWSM) at Carnegie Mellon University, replicating OpenAI Whisper-style training using public data and our open-source toolkit ESPnet. Our models exhibit explainable behaviors due to their transparent development. We also investigate scaling laws and emergent capabilities in speech foundation models by studying model and data size impacts within the OWSM suite. This presentation will discuss these advancements and the research challenges they present to the speech and audio community, emphasizing open collaboration and transparency to enhance accessibility and interpretability in speech processing technologies.</p>     
      </div>
    </li>

      <h4>[Feb 20th, 2025]</h4>
      <li>
        <b>
            <a href="https://www.isca-archive.org/interspeech_2024/shi24_interspeech.pdf"> Singing Voice Synthesis: Data curation, Modeling, and Evaluation </a>
        </b>
        <br> Presenter:<a href="http://shijt.site/"><u> Jiatong Shi </u></a> Carnegie Mellon University
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#feb20bio" role="button" aria-expanded="false" aria-controls="collapseExample">
         Speaker Bio
        </a>
        <div class="collapse" id="feb20bio">
          <div class="card card-body">
            <p><u>Jiatong Shi</u> is a Ph.D. candidate in the Language Technologies Institute at Carnegie Mellon University, advised by Dr. Shinji Watanabe. His research focuses on speech representation learning and its applications across various speech processing tasks. He has authored over 70 publications in leading speech and machine learning conferences and has received multiple prestigious honors, including the Best Paper Award at ISCA Interspeech 2024, the Best Paper Award at EMNLP 2024, and the CMU Presidential Fellowship. Jiatong is also a strong advocate for open-source research, making significant contributions to major toolkits such as ESPnet, Muskits, and VERSA. He has played a key role in curating and releasing influential open datasets, including ML-SUPERB, SingMOS, KiSing, and several endangered language corpora, which have driven advancements in speech and music processing.</p>
          </div>
        </div>
        <br> 
        <a href="https://www.isca-archive.org/interspeech_2024/shi24_interspeech.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://www.isca-archive.org/interspeech_2024/wu24q_interspeech.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
        <a href="https://www.isca-archive.org/interspeech_2024/tang24c_interspeech.html"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
        <a href="https://arxiv.org/abs/2406.10911"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
        <a href="https://github.com/shinjiwlab/versa"><img src="https://img.shields.io/badge/Github-link-lightgrey"></a>
        <a href="https://www.youtube.com/watch?v=AiFTBrkfO18"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/SVS-work-overview-0220.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>    
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#feb20" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract
          </a>
      <div class="collapse" id="feb20">
      <div class="card card-body">
      <p>Singing voice synthesis (SVS) has emerged as a rapidly evolving research area. However, achieving high-quality and expressive singing synthesis remains a challenging task, requiring large-scale curated datasets, effective modeling strategies, and robust evaluation frameworks. This talk will provide a comprehensive overview of the key components of SVS research, covering data curation, model development, and evaluation methodologies. We will introduce ACE-Opencpop and ACE-KiSing, two large-scale singing voice datasets designed to support diverse SVS applications. On the modeling side, we will explore TokSing, a discrete token-based SVS approach, and SingOMD, which leverages multi-resolution discrete representations to enhance synthesis quality. In terms of evaluation, we will discuss the Interspeech 2024 Challenge on Speech Processing Using Discrete Units, the SingMOS dataset for MOS prediction, and VERSA, a versatile evaluation toolkit for speech, audio, and music processing. By bridging data, modeling, and evaluation, this talk aims to provide insights into the current advancements and challenges in SVS research, highlighting emerging directions for improving naturalness, expressiveness, and overall synthesis quality.</p>     
      </div>
      </li>
      
      <h4>[Feb 13th, 2025]</h4>
      <li>
        <b>
            <a href="https://minjekim.com/wp-content/uploads/jasa2024_skim.pdf"> Scalable and Efficient Speech Enhancement </a>
        </b>
        <br> Presenter:<a href="https://minjekim.com/"><u> Minje Kim</u></a> University of Illinois at Urbana-Champaign
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#feb13bio" role="button" aria-expanded="false" aria-controls="collapseExample">
         Speaker Bio
        </a>
        <div class="collapse" id="feb13bio">
          <div class="card card-body">
            <p><u>Minje Kim</u> is an Associate Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign and a Visiting Academic at Amazon Lab126. Prior to that, he was an Associate Professor at Indiana University. He earned his Ph.D. in CS from UIUC after working as a researcher at ETRI, a national lab in Korea (2006–2011). His research focuses on developing machine learning models for speech and audio problems. He has been recognized with various awards, including the NSF Career Award (2021), the Indiana University Trustees Teaching Award (2021), and the IEEE SPS Best Paper Award (2020), among others. He is the Chair of the IEEE SPS Audio and Acoustic Signal Processing Technical Committee, a Senior Area Editor for IEEE SPL and IEEE/ACM TASLP, an Associate Editor for EURASIP JASMP, and a Consulting Associate Editor for IEEE OJSP. He is also on the program committees of many machine learning and audio/speech conferences, including NeurIPS, ICLR, AAAI, ICASSP, Interspeech, ISMIR, etc. He holds over 50 patents as an inventor.</p>
          </div>
        </div>
        <br> 
        <a href="https://minjekim.com/wp-content/uploads/jasa2024_skim.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://minjekim.com/wp-content/uploads/waspaa2021_asivaraman.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://minjekim.com/wp-content/uploads/taslp2022_skim.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://minjekim.com/research-projects/sese/"><img src="https://img.shields.io/badge/website-blue"></a>
        <a href="https://minjekim.com/research-projects/bloom-net/"><img src="https://img.shields.io/badge/website-blue"></a>
        <a href="https://www.youtube.com/watch?v=Olbv6fBu63w"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/ConvAI_RG_Minje_20250213.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#feb13" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract
          </a>
      <div class="collapse" id="feb13">
      <div class="card card-body">
      <p>Recent advances in single-channel speech enhancement have yielded substantial performance gains but often at the cost of prohibitive model sizes or inference complexity. In this talk, I present a three-part framework addressing these challenges by unifying speaker-agnostic compression, personalized modeling, and scalable architectures. First, we discuss speaker-agnostic model compression via low-bit quantization. Drawing on Bitwise Neural Networks (BNN) and Incremental Binarization on RNNs, we show how feedforward and recurrent networks can be quantized to binary parameters, thereby drastically reducing computational complexity with only a minor degradation in enhancement quality. We also explore discriminative hashing approaches (e.g., Boosted Locality Sensitive Hashing) to represent audio spectra as highly compact binary codes—enabling efficient lookups for source separation in resource-constrained devices. Next, we shift toward personalized speech enhancement—a speaker-aware model compression paradigm. Here, zero-shot approaches that exploit speaker embeddings or knowledge distillation serve to adapt compact models on-the-fly without requiring any clean speech data from the user. By selecting or distilling knowledge from large teacher networks, these personalized systems can adapt to new speakers or recurring noise conditions at test time, thus bridging the performance gap between large, general-purpose models and lightweight, specialized models. Finally, we introduce scalable and efficient enhancement architectures. Building on blockwise optimization (BLOOM-Net) and modified cold diffusion, we design flexible models whose internal blocks or iterative steps can be selectively engaged based on real-time resource constraints. This scalability lets the same network accommodate diverse deployment scenarios—ranging from low-power embedded devices to higher-capacity servers—while maintaining strong enhancement performance. </p>     
      </div>
      </li>

      <h4>[Feb 6th, 2025]</h4>
      <li>
        <b>
            <a href={{ "assets/slides/Finetune_Foundation.pdf" | prepend: site.baseurl }}> Teaching Foundation Models New Skills: Insights and Experiences</a>
        </b>
        <br> Presenter:<a href="https://speech.ee.ntu.edu.tw/~hylee/index.php"><u> Hung-yi Lee</u></a> National Taiwan University
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#feb06bio" role="button" aria-expanded="false" aria-controls="collapseExample">
         Speaker Bio
        </a>
        <div class="collapse" id="feb06bio">
          <div class="card card-body">
            <p> <u>Hung-yi Lee</u> is a professor of the Department of Electrical Engineering at National Taiwan University (NTU), with a joint appointment at the Department of Computer Science & Information Engineering. His recent research focuses on developing technology that can reduce the requirement of annotated data for speech processing (including voice conversion and speech recognition) and natural language processing (including abstractive summarization and question answering). He won the Salesforce Research Deep Learning Grant in 2019, the AWS ML Research Award in 2020, the Outstanding Young Engineer Award from The Chinese Institute of Electrical Engineering in 2018, the Young Scholar Innovation Award from Foundation for the Advancement of Outstanding Scholarship in 2019, Ta-You Wu Memorial Award from Ministry of Science and Technology of Taiwan in 2019, and The 59th Ten Outstanding Young Person Award in Science and Technology Research & Development of Taiwan.</p>
          </div>
        </div>
        <br> 
        <!-- <a href=""><img src="https://img.shields.io/badge/Paper-link-important"></a>  -->
        <a href="https://www.youtube.com/watch?v=HQpE0rmhVHI&list=PLmCexrvai6I6BCAxvCFGhM3vau1IyOfp9"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/Finetune_Foundation.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
              
    
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#feb06" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract
          </a>
      <div class="collapse" id="feb06">
      <div class="card card-body">
      <p>In today's landscape of natural language processing (NLP) and speech processing, developing applications often begins with fine-tuning a foundation model. However, teaching a foundation model new skills is not as straightforward as it seems. Despite the sophistication of current models, introducing new capabilities can often impair their original functions, a phenomenon known as catastrophic forgetting. While experience replay is a common solution, the lack of open-source training data for models like LLaMA poses challenges for continuous training. This talk will delve into recent research on fine-tuning language models, including their spoken counterparts, focusing on preserving their initial capabilities. This talk will also share some benchmarks related to the ongoing fine-tuning of foundation models.</p>     
      </div>
      </li>

      <h4>[Jan 23th, 2025]</h4>
    <li>
      <b>
          <a href="https://arxiv.org/abs/2406.19674v1"> Foundational Speech Models and Their Efficient Training with NVIDIA NeMo </a>
      </b>
      <br> Presenter: <a href="https://scholar.google.com/citations?hl=en&user=5YUtDtEAAAAJ&view_op=list_works&sortby=pubdate"><u>Piotr Żelasko</u></a> Nvidia
      <a class="btn btn-info btn-xs" data-toggle="collapse" href="#jan23bio" role="button" aria-expanded="false" aria-controls="collapseExample">
       Speaker Bio
      </a>
      <div class="collapse" id="jan23bio">
        <div class="card card-body">
          <p><u>Piotr Żelasko</u> is a principal research scientist at NVIDIA NeMo. He received his PhD at AGH-UST in Cracow, Poland, and held a research scientist position at JHU’s Center for Language and Speech Processing. Piotr is a co-author of the next-generation Kaldi framework known as k2. His current interests are multi-task, multilingual, and multimodal models involving speech, and training and inference optimization.</p>
        </div>
      </div>
      <br> 
      <a href="https://arxiv.org/abs/2406.19674v1"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
      <a href="https://arxiv.org/abs/2409.13523"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
      <a href="https://arxiv.org/abs/2406.19954"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
      <a href="https://arxiv.org/abs/2310.09424"><img src="https://img.shields.io/badge/Paper-link-important"></a>
      <a href="https://www.youtube.com/watch?v=Zuth3RrMkY8&list=PLmCexrvai6I6BCAxvCFGhM3vau1IyOfp9"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
      <a href={{ "assets/slides/EXTERNAL_Foundational_speech_models_and_their_efficient_training_with_NVIDIA_NeMo_Jan_2025.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
      <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#jan23" role="button" aria-expanded="false" aria-controls="collapseExample">
        Abstract
        </a>
    <div class="collapse" id="jan23">
    <div class="card card-body">
    <p>
      This talk gives an overview of recent developments by NVIDIA NeMo team. We introduce Canary-1B, an open state-of-the-art speech recognition and translation model, and discuss the details of its training: synthetic data generation and efficient dataloading approach that scales to arbitrarily sized datasets. We demonstrate how Canary-1B training was further optimized to decrease the required number of GPUs by 4x with 2D bucketing and batch size optimizer techniques. Finally, we provide a brief overview of SALM and BESTOW architectures for SpeechLLMs and highlight our progress on efficient multimodal SpeechLLM training (EMMETT).
    </p>     
    </div>
  </li>
      <h4>[Jan 16th, 2025]</h4>
      <li>
        <b>
            <a href="https://www.isca-archive.org/interspeech_2024/shi24g_interspeech.pdf"> Improving Universal Access to Modern Speech Technology </a>
        </b>
        <br> Presenter: <a href="https://martijnbartelds.nl/"><u>Martijn Bartelds</u></a> Stanford University
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#jan16bio" role="button" aria-expanded="false" aria-controls="collapseExample">
         Speaker Bio
        </a>
        <div class="collapse" id="jan16bio">
          <div class="card card-body">
            <p><u>Martijn Bartelds</u> is a Postdoctoral Scholar at Stanford University, advised by Dan Jurafsky.
            His research focuses on multilingual speech and language processing, with a particular interest in understanding where language variety and dialect information is encoded in neural speech models, benchmarking, and model training.
            He received his PhD with the highest distinction from the University of Groningen, where his thesis was nominated for the university's best thesis award.
            He also received a prestigious NWO Rubicon fellowship and was a visiting researcher at Delft University of Technology and the University of Pennsylvania.</p>
          </div>
        </div>
        <br> 
        <a href="https://www.isca-archive.org/interspeech_2024/shi24g_interspeech.pdf"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://arxiv.org/abs/2502.01777"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
        <a href="https://www.youtube.com/watch?v=ZPYIb6X-dko&list=PLmCexrvai6I6BCAxvCFGhM3vau1IyOfp9&index=2"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/bartelds_mila.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>

        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#jan16" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract
          </a>
      <div class="collapse" id="jan16">
      <div class="card card-body">
      <p>State-of-the-art speech recognition systems do not work well for many languages, limiting the digital participation of many speakers worldwide. To address this challenge, we need both better ways to reliably measure speech model performance, and new algorithms for bridging this performance gap. In this talk, I propose solutions to both these problems, beginning with ML-SUPERB 2.0, a new benchmark to evaluate multilingual speech models on language identification and automatic speech recognition (ASR) across languages and datasets. Indeed, our benchmark reveals large differences in ASR performance between languages, regardless of the modeling approach used. To mitigate this, I introduce a new model training objective based on distributionally robust optimization. Our new method reduces ASR performance differences between languages by minimizing the training loss of the worst-performing language. This work paves the way for more equal access to speech technology for speakers of all languages. </p>     
      </div>
    </li>

      <h4>[Jan 9th, 2025]</h4>
      <li>
        <b>
            <a href="https://arxiv.org/abs/2402.13071">  Neural Audio Codecs in the Era of Speech LMs </a>
        </b>
        <br> Presenter: <a href="https://hbwu-ntu.github.io/"><u>Haibin Wu</u></a> Microsoft
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#jan9bio" role="button" aria-expanded="false" aria-controls="collapseExample">
         Speaker Bio
        </a>
        <div class="collapse" id="jan9bio">
          <div class="card card-body">
            <p><u>Haibin Wu</u> is a senior researcher at Microsoft, focusing on speech processing. He completed his Ph.D. at National Taiwan University under Prof. Hung-yi Lee. He is a recipient of the Google PhD Fellowship, awarded to only 75 scholars worldwide every year.
              Haibin has published more than 20 first-author papers in top conferences and journals like ICASSP, Interspeech, TASLP, ACL, ASRU, and SLT. He is also a key contributor to S3prl, an open-source speech toolkit with 2.2k GitHub stars.
              He gained industry experience through internships at Microsoft, Meta, Amazon, and Tencent, working on speech generation, enhancement, and model compression. Haibin also conducted research as a visiting student at Tsinghua University and the Chinese University of Hong Kong.
              In addition, Haibin co-organizes the SUPERB and Codec-SUPERB challenges, helping set benchmarks for speech SSL and codec model evaluation.
              </p>
          </div>
        </div>
        <br> 
        <a href="https://arxiv.org/abs/2402.13071"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://codecsuperb.github.io/"><img src="https://img.shields.io/badge/Github-link-lightgrey"></a>
        <a href="https://arxiv.org/abs/2411.18803"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://github.com/ga642381/speech-trident"><img src="https://img.shields.io/badge/Github-link-lightgrey"></a>
        <a href="https://arxiv.org/abs/2406.07237"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://github.com/roger-tseng/CodecFake"><img src="https://img.shields.io/badge/Github-link-lightgrey"></a>
        <a href="https://www.youtube.com/watch?v=BX755dCCGB4"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/codec_presentation.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#jan9" role="button" aria-expanded="false" aria-controls="collapseExample">
           Abstract
          </a>
      <div class="collapse" id="jan9">
      <div class="card card-body">
        <p> Neural audio codecs (NACs) have gained significant attention as essential technologies for audio compression and as foundational components for speech language models. In the era of speech LMs, there are both challenges and opportunities in the codec domain. This talk presents three topics of NACs, including modelling, evaluation and security. 
          This talk introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec provides key benefits, including streaming capability, low computational demands, low bitrate, and a single codebook design, all while delivering high audio quality. 
          Additionally, this talk presents Codec-SUPERB, the first benchmark designed to evaluate codec models in terms of reconstruction quality from both signal-level and application-level perspectives. 
          Finally, this talk presents CodecFake, the first deepfake audio dataset based on codecs. The CodecFake dataset equips models to effectively counter codec-based speech generation systems.
           </p>      
      </div>
    </li>



    </div>
  </div>
</section>
<section id="fall2024" class="some-section">
  <div class="container">
    <div class="row">
      <div class="col-sm-2"></div>
      <div class="col-sm-8">
        <div class="listing" style="clear:both;">
        <div class="left">

      <h3 style="text-align:center">Past Talks, Fall 2024</h3>
      <h4>[Dec 19th, 2024]</h4>
      <li>
        <b>
            <a href="https://arxiv.org/abs/2406.14294"> Discrete Audio Tokens for Multimodal LLMs </a> 
        </b>
        <br> Presenter: <a href="https://sites.google.com/site/mircoravanelli/"><u>Mirco Ravanelli</u></a> Concordia University - Mila
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#dec19bio" role="button" aria-expanded="false" aria-controls="collapseExample">
          Speaker Bio
        </a>
        <div class="collapse" id="dec19bio">
          <div class="card card-body">
            <p><u>Mirco Ravanelli</u> received the Ph.D. (with cum laude distinction) from the University of Trento, Trento, Italy, in December 2017. He is currently an Assistant Professor with Concordia University, Montreal, QC, Canada, an Adjunct Professor with the Universite de Montreal, and a Mila Associate Member. He is the Founder and Leader of the SpeechBrain Project which aims to build an open-source toolkit for conversational AI and speech processing. He is the author or co-author of more than 80 papers on his research interests which include deep learning and conversational AI. He is also an Active Member of the Speech and Machine Learning Communities.</p>
          </div>
        </div>
        <br> 
        <!-- <a href="https://youtu.be/jVbD4U_vWgo"><img src="https://img.shields.io/badge/Youtube-Recording-orange"></a> -->
        <a href="https://arxiv.org/abs/2406.14294"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
        <a href="https://arxiv.org/abs/2406.10735"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://poonehmousavi.github.io/DASB-website/"><img src="https://img.shields.io/badge/website-blue"></a> 
        <a href="https://github.com/speechbrain/benchmarks/tree/main/benchmarks/DASB"><img src="https://img.shields.io/badge/Github-link-lightgrey"></a>
        <a href="https://www.youtube.com/watch?v=2-Dqzg3fuVE"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/Conv_AI_Meeting_Dec_2024.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#dec19" role="button" aria-expanded="false" aria-controls="collapseExample">
            Abstract
          </a>
      <div class="collapse" id="dec19">
      <div class="card card-body">
        <p>Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.</p>          </div>
    </li>
      <h4>[Dec 5th, 2024]</h4>
      <li>
        <b>
         <a href="https://arxiv.org/abs/2403.13086"> Posthoc Explanations for Audio Models </a>
        </b>
        <br> Presenter: <a href="https://ycemsubakan.github.io/"><u>Cem Subakan</u></a> Université Laval - Mila
        <a class="btn btn-info btn-xs" data-toggle="collapse" href="#dec5bio" role="button" aria-expanded="false" aria-controls="collapseExample">
          Speaker Bio
        </a>
        <div class="collapse" id="dec5bio">
          <div class="card card-body">
          <p><u>Cem Subakan</u> is an assistant prof. at the computer science department of Laval University, an affiliate assistant prof. at Concordia University and also an associate academic member at Mila. His research is on machine learning for speech and audio, recently focusing more on explainable machine learning. He recently co-organized the explainable AI for speech and audio workshop at ICASSP 2024, and will be a general chair for the IEEE MLSP 2025 conference.</p>
          </div>
        </div>
        <br> 
        <a href="https://arxiv.org/abs/2403.13086"><img src="https://img.shields.io/badge/Paper-link-important"></a>  
        <a href="https://arxiv.org/abs/2409.08655"><img src="https://img.shields.io/badge/Paper-link-important"></a> 
        <a href="https://arxiv.org/abs/2405.17615v1"><img src="https://img.shields.io/badge/Paper-link-important"></a>
        <a href="https://francescopaissan.it/lmac//"><img src="https://img.shields.io/badge/website-blue"></a> 
        <a href="https://www.youtube.com/watch?v=6ErU-uZ9D_Y"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
        <a href={{ "assets/slides/lmac_convairg.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
        <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#dec5" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract </a>
      <div class="collapse" id="dec5">
      <div class="card card-body">
        <p>He will discuss his recent work on generating explanations for audio models. While deep learning models excel at achieving high performance, they often function as black boxes, offering little transparency into their decision-making processes. His aim in this line of work is to develop methods that produce listenable explanations for these black-box audio models without compromising their original performance. Through several metrics, he demonstrates that the explanations generated by his approach remain faithful to the original model and are both listenable and understandable.   </p>
     </div>
    </li>
        <h4>[Nov 21th, 2024]</h4>
        <li>
          <b>
            <a href={{ "/assets/publications/2010_machine_readable_dictionaries/PARAMETER_AVERAGING_IS_ALL_YOU_NEED_TO_PREVENT_FORGETTING.pdf" | prepend: site.baseurl }}> PARAMETER AVERAGING IS ALL YOU NEED TO PREVENT FORGETTING </a>
          </b>
          <br> Presenter: <a href="https://massey-plantinga.com/"><u>Peter Plantinga</u></a> McGill University
          <a class="btn btn-info btn-xs" data-toggle="collapse" href="#nov21bio" role="button" aria-expanded="false" aria-controls="collapseExample">
            Speaker Bio
          </a>
          <div class="collapse" id="nov21bio">
            <div class="card card-body">
            <p> <u>Peter Plantinga</u> is a Postdoctoral Researcher at McGill University’s Department of Neurology and Neurosurgery, where his research leverages speech and audio data to develop biomarkers for neurodegenerative diseases. With a long-standing passion for applying AI to assistive technologies, Peter has published extensively on enhancing speech intelligibility in noisy environments for both human listeners and automated systems. He is a core developer of the open-source SpeechBrain toolkit, widely used in the speech processing and conversational AI communities, and previously led speech AI projects at JPMorganChase’s Machine Learning Center of Excellence, contributing to several patents in conversational AI technologies. Peter’s current work sits at the intersection of neuroscience and AI, aiming to advance the understanding and treatment of different neurological disorders through innovations in interpretable machine learning for voice analysis.</p>
            </div>
          </div>
          <br> 
          <a href={{ "/assets/publications/2010_machine_readable_dictionaries/PARAMETER_AVERAGING_IS_ALL_YOU_NEED_TO_PREVENT_FORGETTING.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Paper-link-important"></a> 
          <a href="https://www.youtube.com/watch?v=xOY3DeCkbPc"><img src="https://img.shields.io/badge/Youtube-Recording-red"></a>
          <a href={{ "/assets/slides/Parameter_Averaging_is_Al_You_Need_to_Prevent_Forgetting.pdf" | prepend: site.baseurl }}><img src="https://img.shields.io/badge/Slides-green"></a>
          <a class="btn btn-primary btn-xs" data-toggle="collapse" href="#nov21" role="button" aria-expanded="false" aria-controls="collapseExample">
          Abstract </a>
        <div class="collapse" id="nov21">
        <div class="card card-body">
        <p> Continual learning in end-to-end automatic speech recognition (E2E-ASR) often suffers from catastrophic forgetting, where fine-tuning leads to significant performance degradation on previously seen data. While adapters offer a way to switch between fine-tuned models, they still underperform in unseen domains—a challenge when the input domain is unknown. We propose a method that reduces forgetting to just 3.4%, significantly outperforming fine-tuning strategies like LoRA, which exhibits a 49% forgetting rate. By linearly interpolating the parameters of multiple models fine-tuned from the same generalist model, we achieve a unified model that excels across diverse datasets. Moreover, this model can be iteratively fine-tuned and averaged while maintaining low forgetting rates. Our experiments demonstrate the robustness of this approach across various datasets and models, presenting a promising solution for continual learning in E2E-ASR.</p>
        </div>
    </li>

    </div>
  </div>
</section>





<section id="organizers" class="some-section">
    <div class="container">
      <div class="row">
    <div class="col-sm-10"></div>
    <div class="col-sm-1-">
    <div class="listing" style="clear:both;">
      <div class="left">
        <h3 style="text-align:center"><b>Organizers</b> </h3>
        <div class="container">
        <div class="row">

          <div class="col-sm-5">	
            <div class="ratio ratio-4x3">
                <img src="assets/profile-pics/profile.jpg" width="70%" alt="profile picture" class="img-rounded" style="display: block; margin: 0 auto;"/>

                <div class="caption text-center"> <h4><a href="https://poonehmousavi.github.io/">Pooneh Mousavi</a></div></h4>
                <p>
                  <u>Pooneh Mousavi</u> (she/her)  is a computer science PhD student at Mila and Concordia University, supervised by Professor Mirco Ravanelli. She has a broad interest in deep learning for Conversational AI. Her research focuses on discrete self-supervised learning for speech and audio, exploring its potential to bridge audio and language models. She is also one of the main contributors to the SpeechBrain project, a popular open-source conversational AI toolkit.
                  <br> 
                  <a href="https://poonehmousavi.github.io/">Website</a>, <a href="https://scholar.google.com/citations?user=QkuComwAAAAJ&hl=en">Google Scholar</a>, <a href="https://www.linkedin.com/in/pooneh-mousavi">Linkedin</a>
              </p>
            </div>

          </div>
          <div class="col-sm-5">
          <div class="ratio ratio-4x3">
            <!-- <img src="images/farimah.png" width="82.5%" alt="profile picture" class="img-responsive"/> -->
            <img src="assets/profile-pics/HIba_pic.jpeg" width="70%" alt="profile picture" class="img-rounded" style="display: block; margin: 0 auto;"/>

            <div class="caption text-center"> <h4><a href="https://www.linkedin.com/in/hiba-akhaddar-5a252b204">Hiba Akhaddar</a></div></h4>
            <p>
              <u>Hiba Akhaddar</u> (she/her)  is a master’s student majoring in Computer Science at Concordia University and Mila. She is supervised by Pr. Tristan Glatard and Pr. Mirco Ravanelli. Her interests revolve around the applications of Deep Learning in the Medical field. She works on the detection and progression of Parkinson’s Disease from speech.
              <br> 
              <a href="https://mila.quebec/en/directory/hiba-akhaddar/">Website</a>, <a href="https://www.linkedin.com/in/hiba-akhaddar-5a252b204">Linkedin</a>
          </p>
        </div>
      </div>
        </div>
      </div>
    </div>
</section>









<script src="js/jquery.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/jquery.easing.min.js"></script>
<script src="js/scrolling-nav.js"></script>
</body>

</html>
